
So here's the generalization, and that's basically
what a generalized linear model means.
I'm going to generalize those two components.
The first one is to relax the random component to be,
for now, anything I want.
y given x does not have to be Gaussian,
but y given x can be any distribution.
And the ones you'll want to keep in mind
are the distributions that we gave names to in the past.
Such as Bernoulli, exponential, Poisson.
But not all of them.
For example, we'll just try to work
with the family of distributions which
are called exponential family.
For example, the uniform distributions
on the interval 0 theta will be excluded from this family.
And so essentially, one way to think about what this family is
the family for which the conditions
of the asymptotic normality of the central limit
theorems uphold, and the other ones
won't be part of this family.
Now for the regression function, I
want to also relax the fact that mu of x
should be equal to x transpose beta.
So I don't want to say this, but what
I want is that either f of x transpose beta is
equal to mu of x, or I can take the inverse of this function
and for some reason, that will probably never become clear
for you, is we tend to actually give
a name to the inverse function that I just described.
So what I'm going to take is this function g
of mu of x, which is x transpose beta,
and I'm going to want to map the interval (0,1)
to the entire real line, rather than mapping
the entire real line to the interval (0,1).
I'm just doing it backwards, but it's exactly the same thing.
So the f that we had on the previous slide, here,
is just g inverse.
And g is just f inverse.

And so now I'm going to do this so that this g, no matter what
mu is, actually is something that I can model
as being x transpose beta.
This g, the one that's the inverse
of the function of the previous slide,
is the one that we call link function.
So be careful, the link function is not the one
that maps x transpose beta to mu,
but it's the one that maps mu to x transpose beta.
So this is important terminology, the link function.
Let's do it one more time.
Mu of x is y, expectation of y given x is equal to little x,
is called the regression function.

All right, so now we have defined two new functions.
A new function, the link function,
and we've allowed our random component
to be any other thing.
And you know, I could have just said
I want to make random component to just be Bernoulli,
and rather than talking about link function
I could just have found one function that
goes from the interval (0,1) to the whole real line.
So here is one example of such a function.
Right, so I have the interval, say, (0,1).
I want a function that goes from the inside and the interior
of (0,1) to the entire real line.
I want it to be invertible so I could take a function that
looks like this, for example, something like
that looks like arctangent with some shift and rescaling,
and that would be one valid candidate for that.
And I could have just called it a day.
But I'm going to do that still at some level of generality.
I'm still going to allow for other distributions for y,
and I'm still going to allow for other possible choices
for a link function.
And the reason is that there's actually many problems
and many data sets that fall into this.